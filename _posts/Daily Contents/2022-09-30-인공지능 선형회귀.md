---
title: "[Daily Contents] 머신 러닝 알고리즘"
categories:
  - Daily Contents
tags:
  - Daily Contents
  # -
toc: true
toc_sticky: true
toc_label: "머신 러닝 알고리즘"
toc_icon: "bookmark"
---

<br>

## 머신 러닝 알고리즘

- Unsupervised Learning
  - Clustering
  - Dimensionality Reduction
- Supervised Learning
  - Classification
  - Regression
- Reinforcement Learning

- 딥러닝 : 인공 신경망, 합성곱 신경망, 적대적 신경망, 순환 신경망
- 강화학습 : Q-러닝, 유전 알고리즘, SARSA

- 지도학습
  - 회귀 : 선형 회귀, 다항 회귀, 라쏘 회귀
  - 분류 : 의사 결정 트리, 로지스틱 회귀, k-최근접 이웃, 서포트 벡터 머신, 나이브 베이즈
- 비지도학습
  - 군집화 : k-평균, 평균 이동, 밀도 기반 공간 군집, 응집
  - 자원 축소 : t-분포 확률적 이웃 임베딩, 주성분 분석

## 선형 회귀

**정의: 종속 변수 y와 한 개 이상의 독립 변수 x와의 선형 상관 관계를 모델링하는 분석 기법**

### 회귀 문제 분류

- 종속변수와 독립변수 수에 따라 분류
- 독립변수, 종속변수 1개 >> 단변량 단순 회귀
- 독립변수 2개 이상 >> 다중 회귀
- 종속변수 2개 이상 >> 다변량

### 선형 회귀로 풀 수 있는 문제들

```
- 메신저에 등록된 친구의 수와 한 주 동안 주고받은 메세지의 수
- 마일리지에 따른 중고차 가격
- 학습시간과 시험점수
- 도심에서부터의 거리에 따른 토지 가격
- 마케팅 비용과 매출 사이의 관계
- 직선형이 아닌 경우
  >> 다항 회귀(Polynomial Regression): 지수(x^n) 적용 가능
  >> 직선형으로 만들어보기: 데이터 변환(transformation)
```

### 싱나는 수학 시간

- 간단하게 표현한 회귀 모델: $Y = wX + b$
  - 좀 더 정확한 표현(단변량 단순 회귀): $Yi=β0+β1Xi+∈i$(오류항)
  - 일반화(다중 회귀): $Y+Xβ+∈$
- 어떻게 최적의 $(w,b)$를 찾을까?
  - 최량적합선 찾기
  - 각 데이터 포인트와의 거리(오류)가 가장 가까운 직선의 방정식

### 수학으로 찾기

- 오차 정의: SSE(Sum of Squared Errors) : $\sum(Y-\hat{Y})^2$
  - $\hat{Y}$: 적합선의 Y의 값
- 공식
  - $w=$${\sum((X-\overline{X})(Y-\overline{Y})}\over\sum(X-\overline{X})^2$
    - $\overline{X}$: X 평균
    - $\overline{Y}$: Y 평균
  - $b=\overline{Y} - w\overline{X}$

### 머신 러닝으로 선형 회귀(이론)

- 손실 함수 또는 비용 함수
  - 예측값과 실제값의 오차
  - 회귀 문제에서는 일반적으로 MSE(Mean Squared Error)를 사용
  - $MSE =$$1\over n$$\sum(Y-\hat{Y})^2$
- 최적화
  - 가장 작은 손실 함수 값을 출력하는 파라미터$(w, b)$찾기
  - 경사하강법 사용

## 더 공부하기

- (책) 파이썬 라이브러리를 활용한 데이터 분석
- (책) 혼자 공부하는 머신러닝 + 딥러닝
- (책) 밑바닥부터 시작하는 데이터 과학
- (책) 파이썬 라이브러리를 활용한 머신러닝
- (책) 파이썬 데이터 사이언스 핸드북
- (웹) https://wikidocs.net/53560 (Pytorch로 시작하는 딥러닝 입문)
- (웹) https://developers.google.com/machine-learning
- (강의) Coursera - Machine Learning(Andrew Ng)
